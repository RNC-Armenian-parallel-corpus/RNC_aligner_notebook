{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgXqs_xmuPgR"
      },
      "source": [
        "# Демо-скрипт для выравнивания параллельных текстов на армянском и русском языках\n",
        "\n",
        "lingtrain-aligner==1.0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Сетап\n",
        "\n",
        "В качестве инпута подаются два txt-файла на армянском и русском. В скрипте предполагается, что эти файлы лежат на уровне скрипта. Если они лежат в другом месте, нужно прописать путь до файла в переменные `am_input` и `ru_input`. Название для файла, получаемого на выходе (html-документ с раскрашенными предложениями и выровненный csv) нужно задать в переменной `project_name`.\n",
        "\n",
        "В идеале хорошо бы разметить файлы на предмет метаинформации: `Лю Ци Синь%%%%%author.`, `Задача трёх тел%%%%%title.` и т.д., иначе заголовок сливается с основным текстом, т.к. между ними нет разделяющего знака препинания. Теги для такой разметки можно посмотреть в \"полезных ссылках ниже\".\n",
        "\n",
        "Результаты обработки сохраняются в папку `result` по умолчанию, но можно поменять название папки установив его в переменной `output_dir`.\n",
        "\n",
        "## Полезные ссылки\n",
        "- Статья про инструмент и метки для указания метинформации https://habr.com/ru/articles/704958/\n",
        "- github проекта https://github.com/averkij/lingtrain-aligner/tree/main\n",
        "- основывалась на скрипте https://colab.research.google.com/drive/1lgmgCJuFAqjEI2zqn9RWPcQuO6rxC80f?usp=sharing#scrollTo=bZ0ZlbNqqjV6"
      ],
      "metadata": {
        "id": "djUwZcwMa1QR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VswIZz8-qAlG",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3506f40d-ae80-495a-c300-8b6c30226892"
      },
      "source": [
        "#@title Установим зависимости\n",
        "\n",
        "!pip install -q -U lingtrain-aligner==1.0.2\n",
        "!pip install -q razdel dateparser sentence_transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.3/68.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.5/315.5 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31z_1uWuqBzA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cc108b8-554e-4ead-bbdd-4ccb9674390f"
      },
      "source": [
        "#@title Импортируем библиотеки\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import datetime\n",
        "\n",
        "from lingtrain_aligner import preprocessor, splitter, aligner, resolver, reader, helper, vis_helper\n",
        "import pandas as pd\n",
        "import itertools"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3LTah8juCUo"
      },
      "source": [
        "# зададим общие переменные\n",
        "output_dir = 'result'\n",
        "if not os.path.exists(output_dir):\n",
        "  os.mkdir(output_dir)\n",
        "\n",
        "models = [\"sentence_transformer_multilingual\", \"sentence_transformer_multilingual_labse\"]\n",
        "model_name = models[0]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Класс для обработки (нужно запустить)\n",
        "\n",
        "# количество итераций для разрешения конфликтов\n",
        "MAX_ITERATIONS = 3 # @param {\"type\":\"integer\"}\n",
        "\n",
        "class pair2Align:\n",
        "    def __init__(self, text_from_name, text_to_name, project_name, lang_from, lang_to, model):\n",
        "\n",
        "      self.success = False\n",
        "\n",
        "      self.text_from_name = text_from_name\n",
        "      self.text_to_name = text_to_name\n",
        "      assert project_name, 'Нужно передать название текста!'\n",
        "      self.project_name = project_name\n",
        "      self.db_path = project_name + '.db'\n",
        "\n",
        "      self.lang_from = lang_from\n",
        "      self.lang_to = lang_to\n",
        "\n",
        "      # конфигурации для формирования html-документа\n",
        "      self.lang_ordered = [\"from\", \"to\"]\n",
        "\n",
        "      # запускаем пайп\n",
        "      self.load_from()\n",
        "      self.load_to()\n",
        "\n",
        "      # проверяем на условную параллельность\n",
        "      # chrf = nonparallel_bool(''.join(self.text_from[:100])[:3000] if lang_to=='ru' else ''.join(self.text_to[:100])[:3000], ''.join(self.text_to[:100])[:3000] if lang_to=='ru' else ''.join(self.text_from[:100])[:3000])\n",
        "      # if chrf < 0.7:\n",
        "      #   print(f'НЕПАРАЛЛЕЛЬНЫЕ ТЕКСТЫ {self.project_name}! chrF =', chrf)\n",
        "      #   return\n",
        "\n",
        "      self.split_by_sent()\n",
        "      self.conflicts = self.align()\n",
        "      self.resolve_conflicts()\n",
        "      self.get_aligned()\n",
        "\n",
        "      self.success = True\n",
        "\n",
        "    def load_from(self):\n",
        "    # Оригинальный текст\n",
        "      with open(self.text_from_name, \"r\", encoding=\"utf8\") as input1:\n",
        "        self.text_from = input1.readlines()\n",
        "      print(f'Загружен оригинальный текст из {self.text_from_name}')\n",
        "\n",
        "    def load_to(self):\n",
        "    # Текст перевода\n",
        "      with open(self.text_to_name, \"r\", encoding=\"utf8\") as input2:\n",
        "        self.text_to = input2.readlines()\n",
        "      print(f'Загружен текст перевода из {self.text_to_name}')\n",
        "\n",
        "    def split_by_sent(self):\n",
        "      # Добавим метки абзацев, они пригодятся позже\n",
        "      text_from_prepared = preprocessor.mark_paragraphs(self.text_from)\n",
        "      text_to_prepared = preprocessor.mark_paragraphs(self.text_to)\n",
        "\n",
        "      self.splitted_from = splitter.split_by_sentences_wrapper(text_from_prepared, lang_from)\n",
        "      self.splitted_to = splitter.split_by_sentences_wrapper(text_to_prepared, lang_to)\n",
        "\n",
        "      print(f'Предложений в оригинальном тексте ({lang_from}):', len(self.splitted_from))\n",
        "      print(f'Предложений в тексте перевода ({lang_to})', len(self.splitted_to))\n",
        "\n",
        "    def align(self):\n",
        "      \"\"\"\n",
        "      for c in conflicts_to_solve:\n",
        "        resolver.show_conflict(db_path, c)\n",
        "      \"\"\"\n",
        "\n",
        "      if os.path.isfile(self.db_path):\n",
        "        os.unlink(self.db_path)\n",
        "\n",
        "      aligner.fill_db(self.db_path, lang_from, lang_to, self.splitted_from, self.splitted_to)\n",
        "\n",
        "      # IMPORTANT: only part (also just below when called)\n",
        "      # batch_ids = [_ for _ in range(0,10)]\n",
        "\n",
        "      aligner.align_db(self.db_path, \\\n",
        "                      model_name, \\\n",
        "                      batch_size=100, \\\n",
        "                      window=40, \\\n",
        "                      # batch_ids=batch_ids, \\\n",
        "                      save_pic=False,\n",
        "                      embed_batch_size=10, \\\n",
        "                      normalize_embeddings=True, \\\n",
        "                      show_progress_bar=True\n",
        "                      )\n",
        "      vis_helper.visualize_alignment_by_db(self.db_path, output_path='viz.png', lang_name_from=self.lang_from, lang_name_to=self.lang_to, batch_size=400, size=(800,800), plt_show=True)\n",
        "\n",
        "      return resolver.get_all_conflicts(self.db_path, min_chain_length=2, max_conflicts_len=MAX_ITERATIONS, batch_id=-1) # conflicts_to_solve, rest -> resolver.get_statistics(conflicts_to_solve), resolver.get_statistics(rest)\n",
        "\n",
        "\n",
        "    def resolve_conflicts(self, steps=5):\n",
        "      batch_id = -1 #выровнять все доступные батчи\n",
        "\n",
        "      for i in range(steps):\n",
        "          conflicts, rest = resolver.get_all_conflicts(self.db_path, min_chain_length=2+i, max_conflicts_len=MAX_ITERATIONS*(i+1), batch_id=batch_id, handle_start=True, handle_finish=True)\n",
        "          resolver.resolve_all_conflicts(self.db_path, conflicts, model_name, show_logs=False)\n",
        "          vis_helper.visualize_alignment_by_db(self.db_path, output_path='viz.png', lang_name_from=self.lang_from, lang_name_to=self.lang_to, batch_size=400, size=(600,600), plt_show=True)\n",
        "\n",
        "          if len(rest) == 0: break\n",
        "      print('Конфликты разрешены')\n",
        "\n",
        "    def save_book(self, output_filename, output_dir=None, custom_styles=[]):\n",
        "      \"\"\"\n",
        "      # можно передать свои правила для оформления книжки\n",
        "      my_style = [\n",
        "          '{\"background\": \"#A2E4B8\", \"color\": \"black\", \"border-bottom\": \"0px solid red\"}',\n",
        "          '{\"background\": \"#FFC1CC\", \"color\": \"black\"}',\n",
        "          '{\"background\": \"#9BD3DD\", \"color\": \"black\"}',\n",
        "          '{\"background\": \"#FFFCC9\", \"color\": \"black\"}'\n",
        "          ]\n",
        "      save_aligned(custom_styles=my_style)\n",
        "      \"\"\"\n",
        "      output_path = os.path.join(output_dir, output_filename +\".html\")\n",
        "      if not os.path.exists(output_dir):\n",
        "        os.mkdir(output_dir)\n",
        "\n",
        "      paragraphs, delimeters, metas, sent_counter = reader.get_paragraphs(\n",
        "          self.db_path, direction=\"to\"\n",
        "      )\n",
        "\n",
        "      reader.create_book(\n",
        "          lang_ordered=self.lang_ordered,\n",
        "          paragraphs=paragraphs,\n",
        "          delimeters=delimeters,\n",
        "          metas=metas,\n",
        "          sent_counter=sent_counter,\n",
        "          output_path=output_path,\n",
        "          template=\"pastel_fill\",\n",
        "          styles=custom_styles,\n",
        "      )\n",
        "\n",
        "    def get_aligned(self):\n",
        "\n",
        "      paragraphs_from, paragraphs_to, meta, _ = reader.get_paragraphs(self.db_path)\n",
        "\n",
        "      sents_from = list(itertools.chain.from_iterable(paragraphs_from['from']))\n",
        "      sents_to = list(itertools.chain.from_iterable(paragraphs_from['to']))\n",
        "\n",
        "      self.aligned_df = pd.DataFrame(data=[sents_from, sents_to], index=[self.lang_from, self.lang_to]).T\n",
        "\n",
        "    @staticmethod\n",
        "    def save_table(df, output_dir, out_filename):\n",
        "      \"\"\"\n",
        "      pair2Align.save_table(proj.aligned_df, output_dir, out_filename=project_name)\n",
        "      \"\"\"\n",
        "      # сохраняем в excel а не в csv на случай, если нужно будет подправить что-то руками\n",
        "      df.to_excel(f'{output_dir}/{out_filename}.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "r9Uem_MwksmL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lingtrain_align(path_from, path_to, project_name, lang_from, lang_to, model_name):\n",
        "  \"\"\"\n",
        "  Выравнивание с помощью lingtrain-aligner\n",
        "  \"\"\"\n",
        "\n",
        "  proj = pair2Align(text_from_name=path_from, text_to_name=path_to, project_name=project_name, lang_from=lang_from, lang_to=lang_to, model=model_name)\n",
        "  # print(proj.success)\n",
        "\n",
        "  # сохраняем таблицу с выровненными предложениями в папку\n",
        "  if proj.success:\n",
        "    pair2Align.save_table(proj.aligned_df, output_dir, out_filename=project_name)\n",
        "  else:\n",
        "    exceptions.append(proj.project_name)\n",
        "\n",
        "def simple_align(path_from, path_to, project_name):\n",
        "  \"\"\"\n",
        "  Простое выравнивание посредством слепления всего файла в одну строку.\n",
        "  Текст на армянском помещается в первый столбец, на русском -- во второй.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(path_from, 'r', encoding='utf8') as am_f:\n",
        "    am_text = ' '.join(am_f.readlines()).strip()\n",
        "\n",
        "  with open(path_to, 'r', encoding='utf8') as ru_f:\n",
        "    ru_text = ' '.join(ru_f.readlines()).strip()\n",
        "\n",
        "  df = pd.DataFrame(data=[am_text, ru_text], index=['hy', 'ru']).T\n",
        "  df = df.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "  df.to_excel(os.path.join(output_dir, project_name+'.xlsx'), index=False)\n",
        "  print('ONELINER')\n"
      ],
      "metadata": {
        "id": "2tK_5fGt1gaz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Перед выравниванием проверяем пары текстов на параллельность и исключаем непараллельные тексты\n",
        "\n",
        "Непосредственно параллельность проверить не получится, поэтому будем опираться нан метрику chrF.\n",
        "\n",
        "Процедура:\n",
        "1) переводим текст на армянском на русский\n",
        "2) считаем chrF\n",
        "3) если chrF меньше 80, то добавляем пару текстов в исключения  "
      ],
      "metadata": {
        "id": "vJXZcZUhjMfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install deep-translator"
      ],
      "metadata": {
        "id": "aRg8biOJfo5B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from deep_translator import GoogleTranslator\n",
        "# from nltk.translate.chrf_score import sentence_chrf\n",
        "\n",
        "# def nonparallel_bool(armenian_true, russian_true):\n",
        "#   translator = GoogleTranslator(source='armenian', target='russian')\n",
        "#   russian_trans = translator.translate(armenian_true)\n",
        "\n",
        "#   chrf_score = sentence_chrf(russian_true, russian_trans)\n",
        "\n",
        "#   return chrf_score"
      ],
      "metadata": {
        "id": "mmloN8cBu2us"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Запускаем обработку пар текстов в цикле"
      ],
      "metadata": {
        "id": "jk2vQud8XhKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# если есть файлы, которые нужно исключить вручную, то перечисляем их в переменной exceptions.\n",
        "# Указываем только основную часть названия -- то есть без расширения и кода языка.\n",
        "# Например, есть файлы 1128107_am.txt и 1128107_ru.txt, которые мы хотим исключить, тогда в список добавляем значение 1128107\n",
        "exceptions = ['1128107']\n",
        "exceptions_file = '_exceptions.txt'  # @param {\"type\":\"string\"}\n",
        "\n",
        "if os.path.exists(exceptions_file):\n",
        "  with open(exceptions_file, 'r+') as file:\n",
        "      # Read all lines and create a list\n",
        "      exceptions.extend(file.read().splitlines())\n",
        "  len('Количество исключений:', exceptions)\n",
        "else:\n",
        "  print('The specified file does not exist!')"
      ],
      "metadata": {
        "id": "9Pk3uakFjcQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29da4804-0581-4a43-c78a-692c08e9550f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The specified file does not exist!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIONAL: вытаскиваем из архива уже выровненные тексты в папку\n",
        "# (если например нужно обработать 200 пар в два дня, можно сохранить прогресс в архив и продолжить обрабатывать остаток в другой день)\n",
        "# ! unzip result_20260109_195034.zip -d result"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Wc3A3MJhYr5m"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# архив с парными текстами без .zip\n",
        "# input_zip = 'liter' # @param {\"type\":\"string\"}"
      ],
      "metadata": {
        "id": "7RH_ecP59brF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# вытаскиваем из архива пары текстов в формате .txt\n",
        "# ! unzip {input_zip}.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "acBgkqN6XKqA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "def backup(exceptions_file, ts, if_download=False):\n",
        "  with open(exceptions_file, 'w') as file:\n",
        "      # Join the list items with a newline character and write to the file\n",
        "      file.write('\\n'.join(exceptions) + '\\n')\n",
        "\n",
        "  zip_filename = output_dir + '_' + ts\n",
        "  shutil.make_archive(zip_filename, 'zip', output_dir)\n",
        "\n",
        "  if if_download:\n",
        "    files.download(zip_filename+'.zip')\n",
        "    files.download(exceptions_file)"
      ],
      "metadata": {
        "id": "mIDFBvfMGD0p"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from types import NoneType\n",
        "import os\n",
        "import re\n",
        "\n",
        "# None, если файлы лежат в корневой папке\n",
        "input_dir = 'liter' # @param {\"type\":\"string\"}\n",
        "output_dir = 'result' # @param {\"type\":\"string\"}\n",
        "done = os.listdir(output_dir)\n",
        "done = list(map(lambda f: f.replace('.xlsx', ''), done))\n",
        "\n",
        "fnames = list(filter(lambda f: f.endswith('.txt') and re.sub('(_am|am_|_ru|ru_)', '', f).replace('.txt', '') not in done+exceptions, os.listdir(input_dir)))\n",
        "fnames = set(map(lambda f: re.sub('(_am|am_|_ru|ru_)', '', f).replace('.txt', ''), fnames))\n",
        "\n",
        "# язык оригинала\n",
        "lang_from = \"ru\" # @param [\"hy\",\"ru\"]\n",
        "# язык перевода\n",
        "lang_to = \"hy\" # @param [\"hy\",\"ru\"]\n",
        "\n",
        "cntr = 0\n",
        "\n",
        "for project_name in fnames:\n",
        "\n",
        "  if cntr % 500 == 0:\n",
        "    backup(exceptions_file=exceptions_file, ts=datetime.datetime.now().strftime('%Y%m%d_%H%M%S'), if_download=False)\n",
        "\n",
        "  print(datetime.datetime.now().strftime('%H:%M:%S'))\n",
        "  print(project_name)\n",
        "\n",
        "  cur_timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "  db_path = f\"{cur_timestamp}.db\"\n",
        "\n",
        "  # название файла на русском (до .txt)\n",
        "  ru_input = f\"{project_name}_ru\"\n",
        "  # название файла на армянском (до .txt)\n",
        "  am_input = f\"{project_name}_am\"\n",
        "  # print(ru_input, am_input)\n",
        "\n",
        "  ru_input = ru_input+'.txt'\n",
        "  am_input = am_input+'.txt'\n",
        "\n",
        "  # print(ru_input, am_input)\n",
        "\n",
        "  try:\n",
        "    lingtrain_align(\n",
        "        path_from=os.path.join(input_dir, am_input) if lang_from == \"hy\" else os.path.join(input_dir, ru_input),\n",
        "        path_to=os.path.join(input_dir, am_input) if lang_to == \"hy\" else os.path.join(input_dir, ru_input),\n",
        "        project_name=project_name,\n",
        "        lang_from=lang_from,\n",
        "        lang_to=lang_to,\n",
        "        model_name=model_name\n",
        "    )\n",
        "  except TypeError:\n",
        "    # такая ошибка выбрасывается, когда файл состоит из одной строки текста\n",
        "    simple_align(\n",
        "        path_from=os.path.join(input_dir, am_input) if lang_from == \"hy\" else os.path.join(input_dir, ru_input),\n",
        "        path_to=os.path.join(input_dir, am_input) if lang_to == \"hy\" else os.path.join(input_dir, ru_input),\n",
        "        project_name=project_name,\n",
        "    )\n",
        "\n",
        "  cntr += 1\n",
        "\n",
        "  print(cntr, 'done')\n",
        "  print('-'*50)\n",
        "\n",
        "backup(exceptions_file=exceptions_file, ts=datetime.datetime.now().strftime('%Y%m%d_%H%M%S'), if_download=True)\n"
      ],
      "metadata": {
        "id": "r5bPe5zaXp6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(len(os.listdir(input_dir)))/2 , len(set(exceptions))+len(done)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Td1A00LaOGci",
        "outputId": "1d992bc0-b873-469c-9ebc-2318f32d2cf1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vis_helper.visualize_alignment_by_db(db_path='arzrum.db', output_path='viz.png', lang_name_from='ru', lang_name_to='hyr', batch_size=1000, size=(800,800), plt_show=True)"
      ],
      "metadata": {
        "id": "ExfsqSlPvkhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm *.db"
      ],
      "metadata": {
        "id": "4Az-JXTgQ_S0"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}